



Parfait, on va faire Ã§a **trÃ¨s concrÃ¨tement**, exactement dans lâ€™ordre que tu demandes, **comme si tu Ã©tais dans ton notebook**, puis on implÃ©mente **`PreprocessingPort` correctement** (flux logique, pas format physique).

---

# 1ï¸âƒ£ Que se passe-t-il quand tu fais `print(dataset)` ?

## Cas 1 â€” Pandas

```python
df = pd.read_csv("nba.csv")
dataset = PandasDataset(df)

print(dataset)
```

### ğŸ‘‰ Ce que Python affiche rÃ©ellement

```text
<infrastructure.dataset.pandas_dataset.PandasDataset object at 0x10f8c2d90>
```

### ğŸ§  Pourquoi ?

Parce que :

* `dataset` **nâ€™est pas un DataFrame**
* câ€™est **un objet mÃ©tier abstrait**
* tu nâ€™as pas dÃ©fini `__repr__` ou `__str__`

ğŸ‘‰ **Et câ€™est NORMAL et SAIN**

Le Domain **ne veut pas afficher les donnÃ©es**,
il veut **les parcourir**.

---

## Cas 2 â€” Spark

```python
spark_df = spark.read.csv("nba.csv", header=True)
dataset = SparkDataset(spark_df)

print(dataset)
```

### ğŸ‘‰ RÃ©sultat

```text
<infrastructure.dataset.spark_dataset.SparkDataset object at 0x10fa12c40>
```

ğŸ‘‰ Exactement la mÃªme chose.

---

## âœ… LA BONNE FAÃ‡ON DE â€œVOIRâ€ LES DONNÃ‰ES

Si tu veux vÃ©rifier ce quâ€™il y a dedans (en notebook) :

```python
for i, row in enumerate(dataset):
    print(row)
    if i == 2:
        break
```

Exemple de sortie :

```python
{'GP': 82, 'MIN': 28.4, 'PTS': 14.2, ...}
{'GP': 12, 'MIN': 6.1, 'PTS': 2.3, ...}
{'GP': 65, 'MIN': 19.8, 'PTS': 8.7, ...}
```

ğŸ‘‰ **VoilÃ  le flux logique**
ğŸ‘‰ Peu importe Pandas ou Spark

---

# 2ï¸âƒ£ MAINTENANT : implÃ©mentation PRO de `PreprocessingPort`

On va le faire **comme il faut** :

* le Domain dÃ©finit **le contrat**
* lâ€™Infra implÃ©mente avec Pandas
* **le flux logique est respectÃ©**

---

## 2.1ï¸âƒ£ Domain â€” `PreprocessingPort`

ğŸ“ `domain/ports/preprocessing_port.py`

```python
from typing import Protocol
from domain.dataset.dataset import Dataset


class PreprocessingPort(Protocol):
    """
    Contrat de preprocessing.
    """

    def preprocess(self, dataset: Dataset) -> Dataset:
        """
        Applique les rÃ¨gles de preprocessing
        et retourne un nouveau Dataset.
        """
        ...
```

ğŸ‘‰ Points clÃ©s :

* entrÃ©e = `Dataset`
* sortie = `Dataset`
* **aucun DataFrame**
* **aucune lib**

---

## 2.2ï¸âƒ£ Infra â€” ImplÃ©mentation Pandas

ğŸ“ `infrastructure/preprocessing/pandas_preprocessing_adapter.py`

```python
import pandas as pd
from domain.ports.preprocessing_port import PreprocessingPort
from domain.dataset.dataset import Dataset
from infrastructure.dataset.pandas_dataset import PandasDataset


class PandasPreprocessingAdapter(PreprocessingPort):
    """
    Preprocessing avec Pandas.
    """

    def preprocess(self, dataset: Dataset) -> Dataset:
        # ğŸ” flux logique â†’ DataFrame (infra seulement)
        df = pd.DataFrame(list(dataset))

        # ğŸ”§ rÃ¨gles mÃ©tier (exemples)
        df = df.dropna()
        df["GP"] = df["GP"].astype(int)
        df["MIN"] = df["MIN"].astype(float)
        df["PTS"] = df["PTS"].astype(float)

        # ğŸ” retour au flux logique
        return PandasDataset(df)
```

ğŸ‘‰ Oui, ici on matÃ©rialise en DataFrame
ğŸ‘‰ Parce que **le preprocessing a souvent besoin de Pandas**
ğŸ‘‰ Mais **Ã§a reste confinÃ© Ã  lâ€™infrastructure**

---

## 2.3ï¸âƒ£ Utilisation dans ton notebook / application

```python
loader = CsvDatasetLoader("nba.csv")
dataset = loader.load()

preprocessor = PandasPreprocessingAdapter()
clean_dataset = preprocessor.preprocess(dataset)

for row in clean_dataset:
    print(row)
    break
```

Sortie :

```python
{'GP': 82, 'MIN': 28.4, 'PTS': 14.2, ...}
```

---

# 3ï¸âƒ£ POURQUOI CETTE APPROCHE EST LA BONNE

### âœ” Tu peux faire :

| Source          | Ã‡a marche ? |
| --------------- | ----------- |
| Pandas CSV      | âœ…           |
| Spark DataFrame | âœ…           |
| SQL demain      | âœ…           |
| Feature Store   | âœ…           |

### âœ” Sans changer :

* le Domain
* les ports
* la logique mÃ©tier

---

# 4ï¸âƒ£ RÃˆGLE Dâ€™OR (Ã€ RETENIR)

> **`print(dataset)` nâ€™est pas censÃ© afficher des donnÃ©es.**
> **Le dataset est un flux, pas une table.**

Si tu veux voir les donnÃ©es :
ğŸ‘‰ tu **itÃ¨res**
ğŸ‘‰ tu **Ã©chantillonnes**

---

## ğŸ”œ SUITE LOGIQUE

Maintenant, on peut :

* ajouter `FeatureEngineeringPort` sur ce flux
* brancher un `TrainerPort`
* faire un **pipeline complet dâ€™entraÃ®nement**
* gÃ©rer Pandas / Spark automatiquement

Si tu veux, on enchaÃ®ne.



Parfait â€” on va faire **exactement la mÃªme chose pour Spark**, **sans rien changer au Domain**, et tu vas voir que **tout ce quâ€™on a construit prend son sens**.

Je vais procÃ©der en **3 temps** :

1. ğŸ‘‰ Ce que `print(dataset)` donne avec Spark (pour bien ancrer)
2. ğŸ‘‰ ImplÃ©mentation **correcte** de `SparkPreprocessingAdapter`
3. ğŸ‘‰ Utilisation concrÃ¨te (notebook / application)

---

# 1ï¸âƒ£ `print(dataset)` avec Spark (rappel rapide)

```python
spark_df = spark.read.csv("nba.csv", header=True, inferSchema=True)
dataset = SparkDataset(spark_df)

print(dataset)
```

### ğŸ‘‰ RÃ©sultat

```text
<infrastructure.dataset.spark_dataset.SparkDataset object at 0x...>
```

âœ” Normal
âœ” Sain
âœ” Identique Ã  Pandas

ğŸ‘‰ **Le Domain ne â€œvoitâ€ pas les donnÃ©es**,
ğŸ‘‰ il **les parcourt**.

Pour voir les donnÃ©es :

```python
for i, row in enumerate(dataset):
    print(row)
    if i == 2:
        break
```

---

# 2ï¸âƒ£ ImplÃ©mentation PRO de `PreprocessingPort` pour Spark

âš ï¸ **Point trÃ¨s important (niveau senior)**
Avec Spark, **le preprocessing doit rester distribuÃ©**.
On **NE DOIT PAS** convertir en Pandas.

---

## 2.1ï¸âƒ£ Le Domain (INCHANGÃ‰)

ğŸ“ `domain/ports/preprocessing_port.py`

```python
from typing import Protocol
from domain.dataset.dataset import Dataset


class PreprocessingPort(Protocol):
    def preprocess(self, dataset: Dataset) -> Dataset:
        ...
```

ğŸ‘‰ Toujours aucune dÃ©pendance technique.

---

## 2.2ï¸âƒ£ Infra â€” `SparkPreprocessingAdapter`

ğŸ“ `infrastructure/preprocessing/spark_preprocessing_adapter.py`

```python
from domain.ports.preprocessing_port import PreprocessingPort
from infrastructure.dataset.spark_dataset import SparkDataset


class SparkPreprocessingAdapter(PreprocessingPort):
    """
    Preprocessing distribuÃ© avec Spark.
    """

    def preprocess(self, dataset: SparkDataset) -> SparkDataset:
        # âš ï¸ Ici on sait que c'est SparkDataset
        df = dataset._df

        # ğŸ”§ rÃ¨gles mÃ©tier (exemples)
        df = df.dropna()

        df = (
            df.withColumn("GP", df["GP"].cast("int"))
              .withColumn("MIN", df["MIN"].cast("double"))
              .withColumn("PTS", df["PTS"].cast("double"))
        )

        return SparkDataset(df)
```

### ğŸ§  Pourquoi câ€™est correct

* Le **Domain** voit `Dataset â†’ Dataset`
* Lâ€™**infra Spark** sait que câ€™est Spark
* Le preprocessing reste **distribuÃ©**
* Pas de `.toPandas()` âŒ
* Pas de `.collect()` âŒ

---

## 2.3ï¸âƒ£ Utilisation (Application / Notebook)

```python
loader = SparkDatasetLoader(
    spark_session=spark,
    path="nba.csv"
)

dataset = loader.load()

preprocessor = SparkPreprocessingAdapter()
clean_dataset = preprocessor.preprocess(dataset)

for row in clean_dataset:
    print(row)
    break
```

Sortie :

```python
{'GP': 82, 'MIN': 28.4, 'PTS': 14.2, ...}
```

ğŸ‘‰ Toujours le **flux logique**
ğŸ‘‰ Toujours la mÃªme interface
ğŸ‘‰ Mais **calcul distribuÃ©**

---

# 3ï¸âƒ£ COMPARAISON PANDAS vs SPARK (TRÃˆS CLAIRE)

| Aspect     | Pandas          | Spark          |
| ---------- | --------------- | -------------- |
| Conversion | `list(dataset)` | âŒ              |
| ExÃ©cution  | en mÃ©moire      | distribuÃ©e     |
| Drop NA    | `df.dropna()`   | `df.dropna()`  |
| Cast       | `.astype()`     | `.cast()`      |
| Retour     | `PandasDataset` | `SparkDataset` |
| Domain     | identique       | identique      |

ğŸ‘‰ **Le Domain ne voit AUCUNE diffÃ©rence.**

---

# 4ï¸âƒ£ POINT ARCHI IMPORTANT (QUE TU DOIS RETENIR)

> âŒ Le Domain ne choisit PAS Pandas ou Spark
> âœ… Lâ€™Application choisit lâ€™implÃ©mentation
> âœ… Lâ€™Infra exÃ©cute

Exemple Application :

```python
if use_spark:
    loader = SparkDatasetLoader(...)
    preprocessor = SparkPreprocessingAdapter()
else:
    loader = CsvDatasetLoader(...)
    preprocessor = PandasPreprocessingAdapter()
```

ğŸ‘‰ **Aucun `if` dans le Domain.**

---

# ğŸ§  PHRASE Ã€ GRAVER (TRÃˆS IMPORTANTE)

> **Le preprocessing est une rÃ¨gle mÃ©tier,
> mais son exÃ©cution est un dÃ©tail technique.**

---

## ğŸ”œ PROCHAINE Ã‰TAPE LOGIQUE

Maintenant quâ€™on a :

* `Dataset` abstrait
* PandasDataset
* SparkDataset
* PreprocessingPort + 2 adapters

On peut :
ğŸ‘‰ faire `FeatureEngineeringPort` multi-backend
ğŸ‘‰ brancher un `TrainerPort` Spark / sklearn
ğŸ‘‰ construire un pipeline dâ€™entraÃ®nement complet

Dis-moi ce que tu veux faire ensuite.


Parfait.
On va faire **FeatureEngineeringPort multi-backend (Pandas + Spark)** **proprement**, **sans casser le Domain**, **sans if/else cachÃ©s**, et **comme en vraie prod ML**.

Je vais procÃ©der **pas Ã  pas**, avec **le raisonnement + le code**.

---

# ğŸ¯ OBJECTIF

> Construire un **FeatureEngineeringPort** :

* indÃ©pendant de Pandas / Spark
* utilisable **train & inference**
* avec **2 implÃ©mentations infra**
* basÃ© sur un **flux logique de donnÃ©es**

---

# ğŸ§  RAPPEL DE Lâ€™ARCHI (INCHANGÃ‰E)

```
Domain
 â”œâ”€â”€ dataset/
 â”‚    â””â”€â”€ dataset.py
 â””â”€â”€ ports/
      â””â”€â”€ feature_engineering_port.py

Infrastructure
 â”œâ”€â”€ dataset/
 â”‚    â”œâ”€â”€ pandas_dataset.py
 â”‚    â””â”€â”€ spark_dataset.py
 â””â”€â”€ feature_engineering/
      â”œâ”€â”€ pandas_feature_engineering_adapter.py
      â””â”€â”€ spark_feature_engineering_adapter.py
```

ğŸ‘‰ Le Domain ne change PAS
ğŸ‘‰ On ajoute juste des implÃ©mentations

---

# 1ï¸âƒ£ DOMAIN â€” `FeatureEngineeringPort`

ğŸ“ `domain/ports/feature_engineering_port.py`

```python
from typing import Protocol
from domain.dataset.dataset import Dataset


class FeatureEngineeringPort(Protocol):
    """
    Contrat de feature engineering.
    """

    def build_features(self, dataset: Dataset) -> Dataset:
        """
        Construit les features Ã  partir d'un dataset prÃ©processÃ©.
        Retourne un nouveau Dataset.
        """
        ...
```

### ğŸ”‘ Points clÃ©s

* entrÃ©e = `Dataset`
* sortie = `Dataset`
* **aucune dÃ©pendance technique**
* **aucune notion de DataFrame**

---

# 2ï¸âƒ£ DOMAIN â€” SpÃ©cification des features (TRÃˆS IMPORTANT)

ğŸ“ `domain/features/feature_contract.py`

```python
FEATURE_COLUMNS = [
    "GP",
    "MIN",
    "PTS",
    "FGM",
    "FGA",
    "FG_perc",
    "ThreeP_Made",
    "ThreePA",
    "PTS_PER_MIN"
]
```

ğŸ‘‰ **Le Domain dÃ©cide :**

* quelles features existent
* leur nom
* leur signification

ğŸ‘‰ **Lâ€™Infra dÃ©cide comment les calculer.**

---

# 3ï¸âƒ£ INFRA â€” Pandas Feature Engineering

ğŸ“ `infrastructure/feature_engineering/pandas_feature_engineering_adapter.py`

```python
import pandas as pd

from domain.ports.feature_engineering_port import FeatureEngineeringPort
from domain.dataset.dataset import Dataset
from domain.features.feature_contract import FEATURE_COLUMNS
from infrastructure.dataset.pandas_dataset import PandasDataset


class PandasFeatureEngineeringAdapter(FeatureEngineeringPort):
    """
    Feature engineering avec Pandas.
    """

    def build_features(self, dataset: Dataset) -> Dataset:
        # Flux logique â†’ Pandas
        df = pd.DataFrame(list(dataset))

        # ğŸ”§ Feature engineering
        df["PTS_PER_MIN"] = df["PTS"] / df["MIN"]

        # ğŸ”’ SÃ©lection explicite
        df = df[FEATURE_COLUMNS]

        return PandasDataset(df)
```

### ğŸ§  Pourquoi câ€™est bien

* Pandas reste confinÃ©
* rÃ¨gles mÃ©tier respectÃ©es
* features alignÃ©es train / inference

---

# 4ï¸âƒ£ INFRA â€” Spark Feature Engineering

ğŸ“ `infrastructure/feature_engineering/spark_feature_engineering_adapter.py`

```python
from pyspark.sql.functions import col

from domain.ports.feature_engineering_port import FeatureEngineeringPort
from domain.features.feature_contract import FEATURE_COLUMNS
from infrastructure.dataset.spark_dataset import SparkDataset


class SparkFeatureEngineeringAdapter(FeatureEngineeringPort):
    """
    Feature engineering distribuÃ© avec Spark.
    """

    def build_features(self, dataset: SparkDataset) -> SparkDataset:
        df = dataset._df

        # ğŸ”§ Feature engineering
        df = df.withColumn(
            "PTS_PER_MIN",
            col("PTS") / col("MIN")
        )

        # ğŸ”’ SÃ©lection explicite
        df = df.select(*FEATURE_COLUMNS)

        return SparkDataset(df)
```

### âš ï¸ POINT IMPORTANT

* on **nâ€™utilise PAS** `list(dataset)`
* on reste distribuÃ©
* aucune collecte mÃ©moire

---

# 5ï¸âƒ£ APPLICATION â€” Choix du backend (1 seul endroit)

ğŸ“ `application/run_training.py`

```python
USE_SPARK = True

if USE_SPARK:
    loader = SparkDatasetLoader(...)
    preprocessor = SparkPreprocessingAdapter()
    feature_engineer = SparkFeatureEngineeringAdapter()
else:
    loader = CsvDatasetLoader(...)
    preprocessor = PandasPreprocessingAdapter()
    feature_engineer = PandasFeatureEngineeringAdapter()

dataset = loader.load()
clean_dataset = preprocessor.preprocess(dataset)
feature_dataset = feature_engineer.build_features(clean_dataset)
```

ğŸ‘‰ **ZÃ©ro condition dans le Domain**
ğŸ‘‰ **ZÃ©ro duplication logique**
ğŸ‘‰ Backend interchangeable

---

# 6ï¸âƒ£ CE QUE LE DOMAIN â€œVOITâ€ (MENTALEMENT)

Le Domain raisonne comme Ã§a :

```text
Dataset â†’ Dataset â†’ Dataset
```

Il ne sait PAS :

* si câ€™est Pandas
* si câ€™est Spark
* oÃ¹ câ€™est stockÃ©
* comment câ€™est exÃ©cutÃ©

ğŸ‘‰ **Il ne voit que des transformations mÃ©tier.**

---

# 7ï¸âƒ£ CE QUE TU AS MAINTENANT (TRÃˆS IMPORTANT)

âœ… Feature engineering multi-backend
âœ… Train / inference alignÃ©s
âœ… ScalabilitÃ© (Spark ready)
âœ… Clean Architecture respectÃ©e
âœ… PrÃªt pour MLOps rÃ©el

---

# ğŸ”‘ PHRASE Ã€ RETENIR

> **Les features sont une dÃ©cision mÃ©tier,
> leur calcul est une dÃ©cision technique.**

---

## ğŸ”œ PROCHAINE Ã‰TAPE LOGIQUE

On peut maintenant :
ğŸ‘‰ faire un `TrainerPort` Pandas / Spark
ğŸ‘‰ gÃ©rer le split train / valid proprement
ğŸ‘‰ construire un pipeline complet de bout en bout
ğŸ‘‰ gÃ©rer lâ€™infÃ©rence temps rÃ©el

Dis-moi oÃ¹ tu veux aller.
